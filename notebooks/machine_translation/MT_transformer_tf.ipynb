{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0086f61",
   "metadata": {
    "id": "a0086f61"
   },
   "source": [
    "# Дисклеймер\n",
    "Эту тетрадку нужно запускать в колабе или в vast.ai. Не мучатесь с установкой библиотек и с обучением на cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b0ea3c",
   "metadata": {
    "id": "e3b0ea3c"
   },
   "outputs": [],
   "source": [
    "# !apt-get install unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d650e9eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d650e9eb",
    "outputId": "16aecd0e-9060-4289-f372-c1a2372d1953",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 5.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.8)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Installing collected packages: tokenizers\n",
      "Successfully installed tokenizers-0.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers matplotlib sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac85a206",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac85a206",
    "outputId": "3b604a02-892c-4828-9f61-ba31bb7aaa61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jedi==0.17.2\n",
      "  Downloading jedi-0.17.2-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 5.3 MB/s \n",
      "\u001b[?25hCollecting parso<0.8.0,>=0.7.0\n",
      "  Downloading parso-0.7.1-py2.py3-none-any.whl (109 kB)\n",
      "\u001b[K     |████████████████████████████████| 109 kB 45.5 MB/s \n",
      "\u001b[?25hInstalling collected packages: parso, jedi\n",
      "  Attempting uninstall: parso\n",
      "    Found existing installation: parso 0.8.3\n",
      "    Uninstalling parso-0.8.3:\n",
      "      Successfully uninstalled parso-0.8.3\n",
      "  Attempting uninstall: jedi\n",
      "    Found existing installation: jedi 0.18.1\n",
      "    Uninstalling jedi-0.18.1:\n",
      "      Successfully uninstalled jedi-0.18.1\n",
      "Successfully installed jedi-0.17.2 parso-0.7.1\n"
     ]
    }
   ],
   "source": [
    "# в vast ai или в последних версия jupyter может не работать автозаполнение, установка этой либы и перезагрука кернела помогает\n",
    "!pip install --upgrade jedi==0.17.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8559a385",
   "metadata": {
    "id": "8559a385"
   },
   "source": [
    "# Транформеры для машинного перевода"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WjM0-Q75xtgk",
   "metadata": {
    "id": "WjM0-Q75xtgk"
   },
   "source": [
    "На huggingface очень много предобученных трансформеров, которые позволят вам решить очень большой процент рабочих/исследовательских задач. Однако бывают ситуации, когда нужной предобученной модели нет или она работает не очень хорошо. Обучить очень большую модель вряд ли получится (нужны видеокарты), но вот для средних моделей и специфичных задач может хватить даже колаба/каггла."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d2aa7",
   "metadata": {
    "id": "f19d2aa7"
   },
   "source": [
    "Давайте обучим свой небольшой трансформер на задаче машинного перевода. Это самая классическая seq2seq задача и подход, который мы размерем, применим и для других аналогичных задач. Для того, чтобы обучить модель решать какую-то другую seq2seq задачу вам нужно будет только подставить другие данные. Под seq2seq вообще при желании можно подвести любую задачу (классификация - это предсказание последовательности длинной 1 или можно предсказыть названия классов текстом). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "iCLqLoLkdang",
   "metadata": {
    "id": "iCLqLoLkdang"
   },
   "outputs": [],
   "source": [
    "# !pip install tokenizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "947b3313",
   "metadata": {
    "id": "947b3313"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import decoders\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38911d06",
   "metadata": {
    "id": "38911d06"
   },
   "outputs": [],
   "source": [
    "# в русскоязычных данных есть \\xa0 вместо пробелов, он может некорректно обрабатываться токенизатором\n",
    "# text = open('opus.en-ru-train.ru.txt').read().replace('\\xa0', ' ')\n",
    "# f = open('opus.en-ru-train.ru.txt', 'w')\n",
    "# f.write(text)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508e8ae0",
   "metadata": {
    "id": "508e8ae0"
   },
   "source": [
    "Данные взяты вот отсюда - https://opus.nlpl.eu/opus-100.php (раздел с отдельными языковыми парами)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "kvUX7tdtc5pZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kvUX7tdtc5pZ",
    "outputId": "3a1609b9-4478-442e-9437-3d7b65544c84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-20 11:18:48--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
      "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
      "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 121340806 (116M)\n",
      "Saving to: ‘opus.en-ru-train.ru’\n",
      "\n",
      "opus.en-ru-train.ru 100%[===================>] 115.72M  26.5MB/s    in 5.1s    \n",
      "\n",
      "2022-04-20 11:18:53 (22.8 MB/s) - ‘opus.en-ru-train.ru’ saved [121340806/121340806]\n",
      "\n",
      "--2022-04-20 11:18:53--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
      "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
      "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 67760131 (65M)\n",
      "Saving to: ‘opus.en-ru-train.en’\n",
      "\n",
      "opus.en-ru-train.en 100%[===================>]  64.62M  20.1MB/s    in 3.2s    \n",
      "\n",
      "2022-04-20 11:18:57 (20.1 MB/s) - ‘opus.en-ru-train.en’ saved [67760131/67760131]\n",
      "\n",
      "--2022-04-20 11:18:57--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
      "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
      "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 305669 (299K)\n",
      "Saving to: ‘opus.en-ru-test.ru’\n",
      "\n",
      "opus.en-ru-test.ru  100%[===================>] 298.50K   684KB/s    in 0.4s    \n",
      "\n",
      "2022-04-20 11:18:58 (684 KB/s) - ‘opus.en-ru-test.ru’ saved [305669/305669]\n",
      "\n",
      "--2022-04-20 11:18:58--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en\n",
      "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
      "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 173307 (169K)\n",
      "Saving to: ‘opus.en-ru-test.en’\n",
      "\n",
      "opus.en-ru-test.en  100%[===================>] 169.25K   528KB/s    in 0.3s    \n",
      "\n",
      "2022-04-20 11:18:59 (528 KB/s) - ‘opus.en-ru-test.en’ saved [173307/173307]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
    "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
    "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
    "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e110ff04",
   "metadata": {
    "id": "e110ff04"
   },
   "outputs": [],
   "source": [
    "en_sents = open('opus.en-ru-train.en').read().lower().splitlines()\n",
    "ru_sents = open('opus.en-ru-train.ru').read().lower().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b8fae3",
   "metadata": {
    "id": "35b8fae3"
   },
   "source": [
    "Пример перевода с английского на русский"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb9b498",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0eb9b498",
    "outputId": "277adddf-1f07-4960-9b0f-b8377f6add0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('so what are you thinking?', 'ну и что ты думаешь?')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sents[-1], ru_sents[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bbf529",
   "metadata": {
    "id": "31bbf529"
   },
   "source": [
    "Как обычно нам нужен токенизатор, а точнее даже 2, т.к. у нас два корпуса. Будем использовать wordpiece токенайзер из библиотеки tokenizers от huggingface. Wordpiece разбивает текст на токены, которые могут быть как целыми словами, так и кусками слов и даже отдельными символами. \n",
    "В токенизатор мы передаем еще несколько специальных символов - UNK (это символ замена для неизвестных слов), CLS - этот токен будет добавлен в начало текста, SEP - этот символ будет добавлен в конце, PAD - это индекс паддинга\n",
    "\n",
    "В машинном переводе с тэгом UNK нужно что-то делать, потому что нельзя в переводном тексте выдавать технических токенов, но это задача для пост обработки. На уровне токенизации мы не можем от нее избваться, всегда будет слова которые наш токенайзер не знает. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be060a4",
   "metadata": {
    "id": "0be060a4"
   },
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer(WordPiece(), )\n",
    "tokenizer_en.normalizer = normalizers.Sequence([Lowercase()])\n",
    "tokenizer_en.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer_en = WordPieceTrainer(\n",
    "          vocab_size=30000, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\"])\n",
    "tokenizer_en.train(files=[\"opus.en-ru-train.en\"], trainer=trainer_en )\n",
    "\n",
    "tokenizer_ru = Tokenizer(WordPiece(), )\n",
    "tokenizer_ru.normalizer = normalizers.Sequence([Lowercase()])\n",
    "tokenizer_ru.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer_ru = WordPieceTrainer(\n",
    "          vocab_size=30000, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\"])\n",
    "tokenizer_ru.train(files=[\"opus.en-ru-train.ru\"], trainer=trainer_ru )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FZLOU_Qe4Tw-",
   "metadata": {
    "id": "FZLOU_Qe4Tw-"
   },
   "source": [
    "Когда мы будет раскодировать предсказанные индексы в слова нам пригодится декодер. Лучше использовать готовый из той же библиотеки. Он сам склеит символьные нграммы в слова. Без него нам нужно было бы писать правила, чтобы избавиться от вот такого"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "EVuGuhHg3-Tq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "EVuGuhHg3-Tq",
    "outputId": "4e66aeac-a7b6-46d0-a849-23e1667e13ce"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'пример текста с ред ##ким словом'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tokenizer_ru.encode('Пример текста с редким словом').tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xWX3xnMUzbdt",
   "metadata": {
    "id": "xWX3xnMUzbdt"
   },
   "outputs": [],
   "source": [
    "tokenizer_en.decoder = decoders.WordPiece()\n",
    "tokenizer_ru.decoder = decoders.WordPiece()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605ea30c",
   "metadata": {
    "id": "605ea30c"
   },
   "source": [
    "### ВАЖНО!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f07127",
   "metadata": {
    "id": "67f07127"
   },
   "source": [
    "Токенизатор - это неотъемлимая часть модели, поэтому не забывайте сохранять токенизатор вместе с моделью. Если вы забудете про это и переобучите токенизатор, то индексы токенов разойдутся и веса модели будут бесполезны. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "496d0ea7",
   "metadata": {
    "id": "496d0ea7"
   },
   "outputs": [],
   "source": [
    "# раскоментируйте эту ячейку при обучении токенизатора\n",
    "# а потом снова закоментируйте чтобы при перезапуске не перезаписать токенизаторы\n",
    "tokenizer_en.save('tokenizer_en')\n",
    "tokenizer_ru.save('tokenizer_ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4661964",
   "metadata": {
    "id": "f4661964"
   },
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer.from_file(\"tokenizer_en\")\n",
    "tokenizer_ru = Tokenizer.from_file(\"tokenizer_ru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0685f9",
   "metadata": {
    "id": "2b0685f9"
   },
   "source": [
    "Переводим текст в индексы вот таким образом. В начало добавляем токен '[CLS]', а в конец '[SEP]'. Если вспомните занятие по языковому моделированию, то там мы добавляли \"\\<start>\" и \"\\<end>\" - cls и sep по сути тоже самое. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc003758",
   "metadata": {
    "id": "dc003758"
   },
   "outputs": [],
   "source": [
    "def encode(text, tokenizer, target=False):\n",
    "    return [tokenizer.token_to_id('[CLS]')] + tokenizer.encode(text).ids + [tokenizer.token_to_id('[SEP]')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff04bf47",
   "metadata": {
    "id": "ff04bf47"
   },
   "source": [
    "Кодируем и паддим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fc2dae1",
   "metadata": {
    "id": "7fc2dae1"
   },
   "outputs": [],
   "source": [
    "X_en = [encode(t, tokenizer_en) for t in en_sents]\n",
    "X_ru = [encode(t, tokenizer_ru, target=True) for t in ru_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "rXFpjdokwIc7",
   "metadata": {
    "id": "rXFpjdokwIc7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "281b5b90",
   "metadata": {
    "id": "281b5b90"
   },
   "outputs": [],
   "source": [
    "# max_len_en = np.mean([len(x) for x in X_en])\n",
    "# max_len_ru = np.mean([len(x) for x in X_ru])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5984886a",
   "metadata": {
    "id": "5984886a"
   },
   "outputs": [],
   "source": [
    "# max_len_en, max_len_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cc0a376",
   "metadata": {
    "id": "5cc0a376"
   },
   "outputs": [],
   "source": [
    "# ограничимся длинной в 20 и 22 (разные чтобы показать что в seq2seq не нужна одинаковая длина)\n",
    "max_len_en, max_len_ru = 20, 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f4f31fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f4f31fa",
    "outputId": "85345521-b21a-4bc1-96e2-028ecd737034"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# важно следить чтобы индекс паддинга совпадал в токенизаторе с value в pad_sequences\n",
    "PAD_IDX = tokenizer_ru.token_to_id('[PAD]')\n",
    "PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "F7yDKRVP7SC5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F7yDKRVP7SC5",
    "outputId": "0852cc24-a000-47dc-f78d-4f58f71a4ea9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.token_to_id('[PAD]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "49ae735e",
   "metadata": {
    "id": "49ae735e"
   },
   "outputs": [],
   "source": [
    "X_en = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "              X_en, maxlen=max_len_en, padding='post', value=tokenizer_en.token_to_id('[PAD]'))\n",
    "\n",
    "X_ru_out = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "              [x[1:] for x in X_ru], maxlen=max_len_ru-1, padding='post', \n",
    "              value=tokenizer_en.token_to_id('[PAD]'))\n",
    "\n",
    "X_ru_dec = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "              [x[:-1] for x in X_ru], maxlen=max_len_ru-1, \n",
    "              padding='post', value=tokenizer_ru.token_to_id('[PAD]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "824dcb29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "824dcb29",
    "outputId": "34899386-a168-4cd3-b063-71e305cc1c22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000000, 20), (1000000, 21))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# миллион примеров \n",
    "X_en.shape, X_ru_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfa4331",
   "metadata": {
    "id": "acfa4331"
   },
   "source": [
    "Разделяем на трейн и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "25fa5f05",
   "metadata": {
    "id": "25fa5f05"
   },
   "outputs": [],
   "source": [
    "X_en_train, X_en_valid, X_ru_dec_train, X_ru_dec_valid, X_ru_out_train, X_ru_out_valid = train_test_split(X_en, \n",
    "                                                                                                      X_ru_dec, \n",
    "                                                                                                      X_ru_out, \n",
    "                                                                                                      test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9faa719",
   "metadata": {
    "id": "d9faa719"
   },
   "source": [
    "Дальше код модели, он взят вот отсюда (с небольшими изменениями) - https://www.tensorflow.org/text/tutorials/transformer\n",
    "\n",
    "Там есть комментарии по каждому этапу"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f045ac37",
   "metadata": {
    "id": "f045ac37"
   },
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "656be820",
   "metadata": {
    "id": "656be820"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"Calculate the attention weights. \"\"\"\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # add the mask to zero out padding tokens\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f4b51870",
   "metadata": {
    "id": "f4b51870"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "            'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # linear layers\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # split heads\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # scaled dot-product attention\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # concatenation of heads\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))\n",
    "\n",
    "        # final linear layer\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5c48cea2",
   "metadata": {
    "id": "5c48cea2"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, PAD_IDX), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "21c0c899",
   "metadata": {
    "id": "21c0c899"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e120bbe5",
   "metadata": {
    "id": "e120bbe5"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "68472627",
   "metadata": {
    "id": "68472627"
   },
   "outputs": [],
   "source": [
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "89dcc42e",
   "metadata": {
    "id": "89dcc42e"
   },
   "outputs": [],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            max_len,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a2f90e7c",
   "metadata": {
    "id": "a2f90e7c"
   },
   "outputs": [],
   "source": [
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8e9f89b8",
   "metadata": {
    "id": "8e9f89b8"
   },
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            max_len,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e356741b",
   "metadata": {
    "id": "e356741b"
   },
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                max_len,\n",
    "                name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "    # mask the future tokens for decoder inputs at the 1st attention block\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "    # mask the encoder outputs for the 2nd attention block\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "    enc_outputs = encoder(\n",
    "      vocab_size=vocab_size[0],\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "      max_len=max_len[0],\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "    dec_outputs = decoder(\n",
    "      vocab_size=vocab_size[1],\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "      max_len=max_len[1],\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size[1], name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6c35ce0f",
   "metadata": {
    "id": "6c35ce0f"
   },
   "outputs": [],
   "source": [
    "L  = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none',)\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    loss = L(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, PAD_IDX), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "747835a8",
   "metadata": {
    "id": "747835a8"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4743513",
   "metadata": {
    "id": "a4743513"
   },
   "source": [
    "Определяем параметры модели. Есть стандартные наборы параметров для обучения маленькой, средней или большой модели. Их можно посмотреть в оригинальной статье про трансформер - https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "Мы возьмем маленький, т.к. он требует меньше ресурсов. НО для практической задачи есть смысл использовать модель побольше, есть статьи в которые показано, что большие трансформеры обучать эффективнее (требуется меньше примеров) -  https://arxiv.org/pdf/2002.11794.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "542fcc43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "542fcc43",
    "outputId": "c9359c72-3dfb-4985-b26b-a832fc6fb080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# small model\n",
    "NUM_LAYERS = 2\n",
    "D_MODEL = 256\n",
    "NUM_HEADS = 8\n",
    "UNITS = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "\n",
    "# average model\n",
    "# NUM_LAYERS = 6\n",
    "# D_MODEL = 512\n",
    "# NUM_HEADS = 8\n",
    "# UNITS = 2048\n",
    "# DROPOUT = 0.1\n",
    "\n",
    "\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "with mirrored_strategy.scope():\n",
    "    model = transformer(\n",
    "        vocab_size=(tokenizer_en.get_vocab_size(),tokenizer_ru.get_vocab_size()),\n",
    "        num_layers=NUM_LAYERS,\n",
    "        units=UNITS,\n",
    "        d_model=D_MODEL,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        max_len=[max_len_en, max_len_ru])\n",
    "\n",
    "#     learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        0.001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "    def accuracy(y_true, y_pred):\n",
    "#         y_true = tf.reshape(y_true, shape=(-1, max_len_ru - 1))\n",
    "        return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint('model_ruen',\n",
    "                                                monitor='val_loss',\n",
    "                                                verbose=1,\n",
    "                                            save_weights_only=True,\n",
    "                                            save_best_only=True,\n",
    "                                            mode='min',\n",
    "                                            save_freq='epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6590325",
   "metadata": {
    "id": "c6590325"
   },
   "source": [
    "Трансформеры - большие модели, обучаться они должны долго. Размер батча здесь уже имеет большое значение: сликом маленький батч приведет к сильному увеличению времени на обучение, но сильно большим его поставить не получится, т.к. модель большая и быстро займет всю видеокарту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "308a8e81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "308a8e81",
    "outputId": "981be9b3-524f-4e0a-fc93-c44bddd4bf3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4750/4750 [==============================] - ETA: 0s - loss: 2.3551 - accuracy: 0.2131\n",
      "Epoch 1: val_loss improved from inf to 1.83132, saving model to model_ruen\n",
      "4750/4750 [==============================] - 1496s 313ms/step - loss: 2.3551 - accuracy: 0.2131 - val_loss: 1.8313 - val_accuracy: 0.2635\n",
      "Epoch 2/100\n",
      "2815/4750 [================>.............] - ETA: 9:51 - loss: 1.7768 - accuracy: 0.2654"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-750b11efd70a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m              \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m              \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m              )\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Обратите внимание на то как данные подаются в модель \n",
    "# помимо текста в модель еще нужно передать целевую последовательность\n",
    "# но не полную а без 1 последнего элемента\n",
    "# а на выходе ожидаем, что модель сгенерирует этот недостающий элемент\n",
    "# мы сравниваем выход из модели с целевой последовательностью уже с этим последним элементом\n",
    "\n",
    "# сдвинутые последовательности создаются выше\n",
    "# X_ru_dec - это переводной текст без последнего элемента\n",
    "# X_ru_out - это переводной текст с последним элементом\n",
    "\n",
    "model.fit((X_en_train, X_ru_dec_train), X_ru_out_train, \n",
    "             validation_data=((X_en_valid, X_ru_dec_valid), X_ru_out_valid),\n",
    "             batch_size=200,\n",
    "             epochs=100,\n",
    "             callbacks=[checkpoint]\n",
    "             )\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61cb6ed",
   "metadata": {
    "id": "d61cb6ed"
   },
   "source": [
    "Можно попробовать перевести рандомное предложение и оценить качество модели самому"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "63762869",
   "metadata": {
    "id": "63762869"
   },
   "outputs": [],
   "source": [
    "def translate(text):\n",
    "    input_ids = encode(text.lower(), tokenizer_en)\n",
    "\n",
    "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                                      [input_ids], maxlen=max_len_en, padding='post')\n",
    "\n",
    "    \n",
    "    \n",
    "    output_ids = [tokenizer_ru.token_to_id('[CLS]') ]\n",
    "    \n",
    "    pred = model((input_ids, tf.cast([output_ids], tf.int32)), training=False)\n",
    " \n",
    "    \n",
    "    while pred.numpy().argmax(2)[0][-1] not in [tokenizer_ru.token_to_id('[SEP]'), \n",
    "                                                            ]:\n",
    "        if len(output_ids) > max_len_ru:\n",
    "            break\n",
    "        output_ids.append(pred.numpy().argmax(2)[0][-1])\n",
    "        pred = model((input_ids, tf.cast([output_ids], tf.int32)), training=False)\n",
    "\n",
    "    return tokenizer_ru.decode(output_ids[1:])\n",
    "    # ' '.join([tokenizer_ru.id_to_token(i) for i in output_ids[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5f16e8b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "5f16e8b5",
    "outputId": "b974fb32-2c36-44bc-8de9-df7007515f58"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'переход'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7d0ee1b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "7d0ee1b8",
    "outputId": "9fd418f9-1636-4b89-f931-599086753df9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'вы можете перевести это предложение?'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"can you translate this sentence?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aa0b57d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "aa0b57d2",
    "outputId": "cdeea84c-8dae-42f4-c63b-b09b2938c697"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'пожалуйста перевести это предложение в россию'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"please translate this sentence into russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fc8dbbbd",
   "metadata": {
    "id": "fc8dbbbd"
   },
   "outputs": [],
   "source": [
    "# translate(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e2071b",
   "metadata": {
    "id": "a5e2071b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "-ZPgGsEl8Cul",
   "metadata": {
    "id": "-ZPgGsEl8Cul"
   },
   "source": [
    "# BLEU\n",
    "\n",
    "Оценивание машинного перевода очень сложная задача. Главная сложность в том, что хороших переводов одного текста может быть несколько, а есть обычно только 1. Технического решения этой проблемы пока не придумали и если нужна достоверная оценка, то используют людей или даже профессиональных переводчиков. Для автоматической оценки используется простая метрика BLEU. Она считает пересечения между двумя переводами на уровне отдельных токенов, биграммов, триграммов и четырехграммов. Чем больше полных пересечений, тем лучше. Это конечно очень сильное упрощение, но ничего лучше пока особо не придумали"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MX9WvnhW_UNV",
   "metadata": {
    "id": "MX9WvnhW_UNV"
   },
   "source": [
    "Реализация bleu есть в nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "YbuJQX9S8E0o",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YbuJQX9S8E0o",
    "outputId": "89d78b1a-8c15-44c3-c54b-7a2fc45dcbea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4548019047027907\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "hypothesis = ['It', 'is', 'a', 'cat', 'at', 'room']\n",
    "reference = ['It', 'is', 'a', 'cat', 'inside', 'the', 'room']\n",
    "\n",
    "\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
    "print(BLEUscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rdv_0kXKH94I",
   "metadata": {
    "id": "Rdv_0kXKH94I"
   },
   "source": [
    "Давайте расчитаем BLEU на 10 примерах из тестового корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "TbLvCxxlFCn6",
   "metadata": {
    "id": "TbLvCxxlFCn6"
   },
   "outputs": [],
   "source": [
    "en_sents_test = open('opus.en-ru-test.en').read().lower().splitlines()\n",
    "ru_sents_test = open('opus.en-ru-test.ru').read().lower().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "Dy0cRSoLFC0k",
   "metadata": {
    "id": "Dy0cRSoLFC0k"
   },
   "outputs": [],
   "source": [
    "translations = []\n",
    "\n",
    "for i in range(10):\n",
    "  translations.append(translate(en_sents_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "Cuj_lcoUFg9W",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cuj_lcoUFg9W",
    "outputId": "c5949743-1846-46b1-b758-e4c9471bc956"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['если только вы здесь не только.',\n",
       " 'я не знаю, как ты это делаешь, и все эти коробки.',\n",
       " 'у нас может быть небольшое чувство посредничества.',\n",
       " 'сколько ему нужно?',\n",
       " 'в.',\n",
       " 'система не только улучшает процесс управления, но также также также способствовало существенное децентрализованным процедурам.',\n",
       " 'ты не любишь курицу с супом?',\n",
       " 'posted : 14 sep 2005, 20 : 31',\n",
       " 'теперь, для минуты, я думал, что он был в хвосте.',\n",
       " '« : 26 октябрь 2017, 06 : 50 : 24 »']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "pRY7yBESGYxa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pRY7yBESGYxa",
    "outputId": "7fbaf103-1e48-41b3-f616-472821d34765"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['только бы не вылететь.',\n",
       " 'и как ты только справляешься, папа, таская эти коробки взад-вперед целый день.',\n",
       " 'возможно, у нас есть небольшое преимущество в переговорах.',\n",
       " 'сколько времени вы будете делать то, что ему нужно?',\n",
       " '1 апреля президент нкр бако саакян принял начальника генштаба вооруженных сил республики армения генерал-полковника юрия хачатурова.',\n",
       " 'г-н приснер также упомянул, что система электронного правосудия не только позволила улучшить процесс ведения дел, но также способствует значительному упорядочению процедур.',\n",
       " '- неплохо, да.',\n",
       " 'posted: 15 dec 2006, 00:07',\n",
       " 'и на минутку я подумал, что за ним могут следить.',\n",
       " '«: 11 октябрь 2011, 17:15:34»']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_sents_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YzWaYs0lIB80",
   "metadata": {
    "id": "YzWaYs0lIB80"
   },
   "source": [
    "BLEU очень сильно зависит от токенизации. В нашем случае это не проблема так как мы оцениваем одну модель. Но если бы мы хотели сравнить несколько, то нужно было бы создавать 1 общий токенизатор и рассчитывать через него"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "hx3kUhHQFhMQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hx3kUhHQFhMQ",
    "outputId": "82262875-3ef2-4232-caf3-36ad77f76320"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "bleus = []\n",
    "\n",
    "for i, t in enumerate(translations):\n",
    "  reference = tokenizer_ru.encode(t).tokens\n",
    "  hypothesis = tokenizer_ru.encode(ru_sents_test[i]).tokens\n",
    "\n",
    "bleus.append(nltk.translate.bleu_score.sentence_bleu([reference], hypothesis,  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "rJDX-Z-3GKDE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rJDX-Z-3GKDE",
    "outputId": "ee22d497-8f53-4522-89c2-aeb5e329cbed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.85331446417219"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(bleus)/len(bleus))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Lwuh1mO8Icw0",
   "metadata": {
    "id": "Lwuh1mO8Icw0"
   },
   "source": [
    "BLEU обычно показывают не от 0 до 1, а от 0 до 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Bf8nU0Y6JKtV",
   "metadata": {
    "id": "Bf8nU0Y6JKtV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MT_transformer_tf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
